# @package _global_

# to execute this experiment run:
# python train.py experiment=latent_kitti_vio

defaults:
  - override /data: latent_kitti_vio
  - override /model: latent_vio_tf
  - override /callbacks: default
  - override /trainer: default

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["latent_kitti_vio", "weighted", "Manifold", "transformer"]

seed: 42

trainer:
  min_epochs: 20
  max_epochs: 200
  log_every_n_steps: 1

data:
  batch_size: 128
  train_loader:
    root_dir: ${paths.data_dir}/kitti_latent_data/train_10
  val_loader:
    root_dir: ${paths.data_dir}/kitti_latent_data/val_10
  test_loader: 
    root_dir: ${paths.data_dir}/kitti_latent_data/val_10

params: 
  v_f_len: 512
  i_f_len: 256
  g_f_len: 0 
  embed_dim: 512 
  hidden_dim: 128
  num_layers: 4
  num_heads: 8
  dropout: 0.0
  imu_dropout: 0.1
  preload_img_feature: True
  use_preload_vi: False
  img_w: 512
  img_h: 256
  seq_len: 11
  max_offset: 0.5,
  layer_norm_eps: 1e-5
  qkv_bias: True
  batch_first: True
  use_temporal_encoding: False
  use_temporal_encoding_inattention: False

model:
  _target_: src.models.weighted_vio_module.WeightedVIOLitModule
  
  net:
    _target_: src.models.components.posemodel.PoseTemporalTransformerEncoder
    params: ${params}

  optimizer:
    lr: 0.0001

  criterion:
    _target_: src.metrics.weighted_loss.RPMGPoseLoss
    base_loss_fn:
      _target_: torch.nn.L1Loss
      reduction: none
    angle_weight: 40
  
  scheduler:
    T_0: 25
    T_mult: 1

  tester:
    seq_len: 11
    use_history_in_eval: True
    imu_dropout: 0.1
    wrapper_weights_path: 'E:/VITF/pretrained_models/vf_512_if_256_3e-05.model'




