import sys
import os
import math
from tqdm import tqdm
import torch
from torch.utils.data import Dataset
from path import Path

sys.path.insert(0, '../')
# from src.data.components.new_KITTI_dataset import KITTI
import torch.nn as nn
import numpy as np
# from src.models.components.new_vsvio import Encoder

def read_gnss_from_text(file_path):
    """
    ä»GPS ENUåæ ‡æ–‡ä»¶åŠ è½½æ•°æ®
    è¿”å›: Nx3 numpyæ•°ç»„
    """
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) < 3:
                continue
            x, y, z = map(float, parts[:3])
            data.append([x, y, z])
    return np.array(data)


class LatentVectorDataset(Dataset):
    def __init__(self, root,root_dir,
                 sequence_length=11,
                 train_seqs=['00', '01', '02', '04', '06', '09'],
                 transform=None):

        self.root = Path(root)
        self.root_dir = Path(root_dir)
        # print(f"self.root path: {self.root}")
        # print(f"self.root_dir path: {self.root_dir}")
        self.sequence_length = sequence_length
        self.train_seqs = train_seqs
        self.make_dataset()

    def make_dataset(self):
        sequence_set = []
        for folder in self.train_seqs:
            gnss = read_gnss_from_text(self.root / 'oxts' / folder / '{}.txt'.format(folder))
            # print(f"Sequence {folder}: GNSS length = {len(gnss)}")
            for i in range(0, len(gnss) - self.sequence_length, self.sequence_length):
                # å–ä¸€æ®µé•¿åº¦ä¸º sequence_length
                segment = gnss[i:i + self.sequence_length]
                # å·®åˆ†ï¼šsegment[t+1] - segment[t]
                gnss_samples = segment[1:] - segment[:-1]  # å½¢çŠ¶ (sequence_length-1, 3)
                sample = {'gnss': gnss_samples}
                sequence_set.append(sample)
        self.samples = sequence_set

    def __getitem__(self, idx):
        latent_vector = np.load(os.path.join(self.root_dir, f"{idx}.npy"))
        gt = np.load(os.path.join(self.root_dir, f"{idx}_gt.npy"))
        rot = np.load(os.path.join(self.root_dir, f"{idx}_rot.npy"))
        w = np.load(os.path.join(self.root_dir, f"{idx}_w.npy"))
        gnss = np.array(self.samples[idx]['gnss'])

        # è½¬æˆ torch tensor å¹¶ä¿è¯ dtype
        latent_vector = torch.from_numpy(latent_vector).float()
        gnss = torch.from_numpy(gnss).float()
        rot = torch.from_numpy(rot).float()
        w = torch.from_numpy(w).float()

        # GT shape squeezeï¼Œç¡®ä¿ (T, 6)
        gt = torch.from_numpy(gt).float()
        if gt.dim() == 4 and gt.size(1) == 1:  # å»æ‰å¤šä½™çš„ç»´åº¦
            gt = gt.squeeze(1)  # [B, 1, T, 6] -> [B, T, 6]
        elif gt.dim() == 3 and gt.size(0) == 1:
            gt = gt.squeeze(0)

        return (latent_vector, gnss, rot, w), gt

    def __len__(self):
        return len(self.samples)


class DoubleLinearQKV(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.q = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
        self.k = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
        self.v = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
    def forward(self, x):
        q = self.q(x)
        k = self.k(x)
        v = self.v(x)
        return q, k, v

class CustomSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        assert embed_dim % num_heads == 0

        self.out_proj = nn.Linear(embed_dim, embed_dim)

    def forward(self, q, k, v):
        B, T, _ = q.size()

        def split_heads(x):
            return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # [B, H, T, head_dim]

        # ä¸å†è°ƒç”¨ .to(self.device) â€”â€” è¾“å…¥å·²å¤„ç†å¥½
        q = split_heads(q)
        k = split_heads(k)
        v = split_heads(v)

        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn_weights = torch.softmax(attn_scores, dim=-1)
        attn_output = torch.matmul(attn_weights, v)
        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, self.embed_dim)
        return self.out_proj(attn_output)

class GNSS_encoder(nn.Module):
    def __init__(self, opt):
        super(GNSS_encoder, self).__init__()

        self.embedding_dim = opt.g_f_len  # è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦
        self.embedding = nn.Linear(3, self.embedding_dim)
        self.qkv = DoubleLinearQKV(self.embedding_dim, 128, self.embedding_dim)
        self.attention = CustomSelfAttention(self.embedding_dim, 8)
        self.ffn = nn.Sequential(
            nn.Linear(self.embedding_dim, 128),
            nn.ReLU(),
            nn.Linear(128, self.embedding_dim)
        )
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.embedding_dim,
            nhead=8,
            dim_feedforward=512,
            dropout=0.1,
            batch_first=True  # ä¿è¯ä¸å† permute
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)
        self.project = nn.Linear(self.embedding_dim, opt.g_f_len)
    def positional_embedding(self, seq_length, device):
        pos = torch.arange(0, seq_length, dtype=torch.float, device=device).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, self.embedding_dim, 2, device=device).float() * -(math.log(10000.0) / self.embedding_dim))
        pos_embedding = torch.zeros(seq_length, self.embedding_dim, device=device)
        pos_embedding[:, 0::2] = torch.sin(pos * div_term)
        pos_embedding[:, 1::2] = torch.cos(pos * div_term)
        return pos_embedding.unsqueeze(0)  # shape: (1, T, D)
    def forward(self, x):
        # x: (B, T, 3)
        B, T, _ = x.size()
        device = x.device
        x = self.embedding(x)  # (B, T, D)
        pos_embedding = self.positional_embedding(T, device)
        x = x + pos_embedding
        q, k, v = self.qkv(x)
        attn_out = self.attention(q, k, v)
        x = x + attn_out
        x = x + self.ffn(x)
        x = self.transformer_encoder(x)  # batch_first=True
        x = self.project(x)
        return x


# ===== å‚æ•°è®¾ç½® =====
class ObjFromDict:
    def __init__(self, dictionary):
        for k, v in dictionary.items():
            setattr(self, k, v)

params = {
    "img_w": 512,
    "img_h": 256,
    "v_f_len": 512,
    "i_f_len": 256,
    "g_f_len": 128,
    "imu_dropout": 0.1,
    "seq_len": 11
}
params = ObjFromDict(params)

class PoseTransformer(nn.Module):
    def __init__(self, input_dim=896, embedding_dim=128, num_layers=2, nhead=8, dim_feedforward=512, dropout=0.1,
                 device='cuda'):
        super(PoseTransformer, self).__init__()
        self.embedding_dim = embedding_dim
        self.num_layers = num_layers
        self.device = device
        self.fc1 = nn.Sequential(
            nn.Linear(input_dim, self.embedding_dim),
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer=nn.TransformerEncoderLayer(
                d_model=self.embedding_dim,
                nhead=nhead,
                dim_feedforward=dim_feedforward,
                dropout=dropout,
                batch_first=True
            ),
            num_layers=num_layers
        )
        self.fc2 = nn.Sequential(
            nn.Linear(self.embedding_dim, self.embedding_dim),
            nn.LeakyReLU(0.1),
            nn.Linear(self.embedding_dim, 6)
        )
        self.to(self.device)

    def positional_embedding(self, seq_length):
        pos = torch.arange(0, seq_length, dtype=torch.float, device=self.device).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, self.embedding_dim, 2, device=self.device).float() * -(
                        math.log(10000.0) / self.embedding_dim))
        pos_embedding = torch.zeros(seq_length, self.embedding_dim, device=self.device)
        pos_embedding[:, 0::2] = torch.sin(pos * div_term)
        pos_embedding[:, 1::2] = torch.cos(pos * div_term)
        return pos_embedding.unsqueeze(0)

    def generate_square_subsequent_mask(self, sz):
        return torch.triu(
            torch.full((sz, sz), float("-inf"), dtype=torch.float32, device=self.device),
            diagonal=1
        )

    def forward(self, batch):
        device = next(self.fc1.parameters()).device
        visual_inertial_features = batch.to(device)
        seq_length = visual_inertial_features.size(1)
        pos_embedding = self.positional_embedding(seq_length).to(device)
        visual_inertial_features = self.fc1(visual_inertial_features)
        visual_inertial_features += pos_embedding
        mask = self.generate_square_subsequent_mask(seq_length).to(device)
        output = self.transformer_encoder(visual_inertial_features, mask=mask)
        output = self.fc2(output)
        return output


class FeatureEncodingModel(nn.Module):
    def __init__(self, params, device='cuda'):
        super().__init__()
        self.device = device
        self.gnss_encoder = GNSS_encoder(params)
        self.pose_transformer = PoseTransformer(input_dim=params.v_f_len + params.i_f_len + params.g_f_len,
                                                device=self.device)
        self.to(self.device)

    def forward(self, LatentVector, gnss):
        gnss = gnss.to(self.device)

        feat_g = self.gnss_encoder(gnss)

        # print(f"LatentVector shape: {LatentVector.shape}")
        # print(f"feat_g shape: {feat_g.shape}")
        feat_all = torch.cat([LatentVector, feat_g], dim=-1)
        # print(f"feat_all shape: {feat_all.shape}")

        output = self.pose_transformer(feat_all)

        return output

# ===== æ•°æ®é›† =====

dataset = LatentVectorDataset(
    "kitti_data",
    root_dir="kitti_latent_data/train_10",
    train_seqs=['00', '01', '02', '04', '06', '09'],
    sequence_length=11
)
loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)

# ===== å®ä¾‹åŒ–æ¨¡å‹ =====
device = 'cuda:0'
model = FeatureEncodingModel(params, device=device)

model_dict = model.state_dict()

# GNSSç¼–ç å™¨æƒé‡
tot_dict_gnss = {k: v for k, v in model_dict.items() if "gnss_encoder" in k}
print(f"GNSSç¼–ç å™¨æƒé‡æ•°é‡: {len(tot_dict_gnss)}")

# PoseTransformeræƒé‡
tot_dict_pose = {k: v for k, v in model_dict.items() if "pose_transformer" in k}
print(f"PoseTransformeræƒé‡æ•°é‡: {len(tot_dict_pose)}")


# ===== ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•° =====
optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)
criterion = nn.L1Loss()

# ===== è®­ç»ƒè®¾ç½® =====
checkpoint_dir = "E:/VITF/pretrained_models/fast_transformer_process_nortf_wrong_plus"
os.makedirs(checkpoint_dir, exist_ok=True)

# resume_path = os.path.join(checkpoint_dir, "model_epoch_60.pth")
start_epoch = 0

# if os.path.exists(resume_path):
#     print(f"ğŸ”„ æ¢å¤è®­ç»ƒï¼šåŠ è½½ {resume_path}")
#     checkpoint = torch.load(resume_path, map_location=device)
#     model.load_state_dict(checkpoint['model_state_dict'])
#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
#     start_epoch = checkpoint['epoch']  # ä»ç¬¬10è½®åç»§ç»­
# else:
#     print("âš ï¸ æœªæ‰¾åˆ°ä¸Šæ¬¡æ¨¡å‹ï¼Œé»˜è®¤ä»å¤´å¼€å§‹è®­ç»ƒ")

# ===== æ­£å¼è®­ç»ƒ =====
model.train()
for epoch in range(start_epoch, start_epoch + 200):  # å†è®­ç»ƒ40è½®
    epoch_loss = 0.0
    print(epoch)
    for i, ((LatentVector, gnss, rot, w), gts) in enumerate(tqdm(loader)):
        LatentVector = LatentVector.to(device).float()
        gnss = gnss.to(device).float()
        gts = gts.to(device).float()

        optimizer.zero_grad()
        pred_pose = model(LatentVector, gnss)
        # print(f"Batch {i} pred_pose shape: {pred_pose.shape}")

        loss = criterion(pred_pose, gts)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()

    avg_loss = epoch_loss / (i + 1)
    print(f"[Epoch {epoch + 1}] Avg Loss: {avg_loss:.6f}")

    torch.save({
        'epoch': epoch + 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': avg_loss
    }, os.path.join(checkpoint_dir, f"model_epoch_{epoch + 1}.pth"))

# ===== æœ€ç»ˆæ¨¡å‹ä¿å­˜ =====
torch.save(model.state_dict(), os.path.join(checkpoint_dir, "gnss_encoder_transformer_final.pth"))
